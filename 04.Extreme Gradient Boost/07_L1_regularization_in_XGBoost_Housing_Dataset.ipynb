{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07. L1 regularization in XGBoost Housing Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPpznfk6pI2XZ9uw4S0HUio",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raj-vijay/ml/blob/master/04.Extreme%20Gradient%20Boost/07_L1_regularization_in_XGBoost_Housing_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaSXqmUns3Wl"
      },
      "source": [
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXXPhQOev2Gf"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "X, y = load_boston(return_X_y=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0eWcgpox5Xj"
      },
      "source": [
        "# Create the training and test sets\n",
        "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcJlRfRs1fdG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "07fa6227-da6d-4292-e9fd-15682f1f11d7"
      },
      "source": [
        "boston_dmatrix = xgb.DMatrix(data=X,label=y)\n",
        "params={\"objective\":\"reg:squarederror\",\"max_depth\":4}\n",
        "l1_params = [1,10,100]\n",
        "rmses_l1=[]\n",
        "for reg in l1_params:\n",
        "  params[\"alpha\"] = reg\n",
        "  cv_results = xgb.cv(dtrain=boston_dmatrix, params=params,nfold=4,\n",
        "  num_boost_round=10,metrics=\"rmse\",as_pandas=True,seed=123)\n",
        "  rmses_l1.append(cv_results[\"test-rmse-mean\"].tail(1).values[0])\n",
        "print(\"Best rmse as a function of l1:\")\n",
        "print(pd.DataFrame(list(zip(l1_params,rmses_l1)), columns=[\"l1\",\"rmse\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best rmse as a function of l1:\n",
            "    l1      rmse\n",
            "0    1  3.461474\n",
            "1   10  3.821152\n",
            "2  100  4.645519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryKRowvIZNA_"
      },
      "source": [
        "**Tuning the number of boosting rounds**\n",
        "\n",
        "Let's start with parameter tuning by seeing how the number of boosting rounds (number of trees build) impacts the out-of-sample performance of the XGBoost model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq_u2GvmZIbp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "929d86dc-6126-4852-e98b-c14bdef25e02"
      },
      "source": [
        "# Create the DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "# Create the parameter dictionary for each tree: params \n",
        "params = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n",
        "\n",
        "# Create list of number of boosting rounds\n",
        "num_rounds = [5, 10, 15]\n",
        "\n",
        "# Empty list to store final round rmse per XGBoost model\n",
        "final_rmse_per_round = []\n",
        "\n",
        "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
        "for curr_num_rounds in num_rounds:\n",
        "\n",
        "    # Perform cross-validation: cv_results\n",
        "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "    \n",
        "    # Append final round RMSE\n",
        "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
        "\n",
        "# Print the resultant DataFrame\n",
        "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
        "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   num_boosting_rounds      rmse\n",
            "0                    5  5.950357\n",
            "1                   10  3.784688\n",
            "2                   15  3.525731\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODH9DzHqZ0iP"
      },
      "source": [
        "**Automated boosting round selection using early_stopping**\n",
        "\n",
        "Now, instead of attempting to cherry pick the best possible number of boosting rounds, it is possible to very easily have XGBoost automatically select the number of boosting rounds for you within xgb.cv(). \n",
        "\n",
        "This is done using a technique called early stopping.\n",
        "\n",
        "Early stopping works by testing the XGBoost model after every boosting round against a hold-out dataset and stopping the creation of additional boosting rounds (thereby finishing training of the model early) if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds. \n",
        "\n",
        "The holdout metric continuously improves up through when num_boost_rounds is reached, then early stopping does not occur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFf7PNiJZpqG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "5b105256-0cc0-48cd-a85b-3eb000196307"
      },
      "source": [
        "# Create your housing DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
        "\n",
        "# Create the parameter dictionary for each tree: params\n",
        "params = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n",
        "\n",
        "# Perform cross-validation with early stopping: cv_results\n",
        "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=50, early_stopping_rounds=10, metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "\n",
        "# Print cv_results\n",
        "print(cv_results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
            "0         17.131356        0.020616       17.223397       0.067773\n",
            "1         12.382814        0.025832       12.619156       0.132110\n",
            "2          9.063983        0.037020        9.505188       0.117293\n",
            "3          6.726435        0.034949        7.339914       0.120754\n",
            "4          5.102424        0.045689        5.954197       0.137525\n",
            "5          3.976571        0.049928        5.043778       0.198929\n",
            "6          3.195956        0.061088        4.436179       0.214428\n",
            "7          2.694113        0.066253        4.066125       0.295735\n",
            "8          2.341284        0.050819        3.837729       0.326883\n",
            "9          2.099043        0.052467        3.667519       0.352621\n",
            "10         1.931933        0.042599        3.573348       0.360856\n",
            "11         1.824035        0.047074        3.510915       0.370727\n",
            "12         1.713532        0.058416        3.469601       0.372425\n",
            "13         1.628078        0.058896        3.441818       0.394071\n",
            "14         1.559479        0.058445        3.403518       0.407603\n",
            "15         1.482433        0.049533        3.367522       0.431987\n",
            "16         1.437707        0.039150        3.351383       0.439357\n",
            "17         1.370599        0.033117        3.350095       0.440003\n",
            "18         1.346623        0.027598        3.349142       0.440584\n",
            "19         1.305355        0.025664        3.334845       0.432809\n",
            "20         1.256261        0.041769        3.315190       0.430103\n",
            "21         1.218349        0.037456        3.305477       0.429272\n",
            "22         1.183180        0.030335        3.302782       0.435573\n",
            "23         1.134491        0.028116        3.306710       0.432787\n",
            "24         1.102499        0.025421        3.309918       0.429725\n",
            "25         1.072964        0.026608        3.304234       0.427132\n",
            "26         1.047389        0.032165        3.298778       0.430648\n",
            "27         1.022132        0.029575        3.293409       0.427261\n",
            "28         0.986890        0.030724        3.288524       0.429088\n",
            "29         0.957547        0.035886        3.290293       0.428384\n",
            "30         0.933272        0.032564        3.293034       0.431208\n",
            "31         0.913488        0.038415        3.290024       0.431993\n",
            "32         0.897809        0.037734        3.286117       0.432014\n",
            "33         0.875283        0.038628        3.287901       0.428564\n",
            "34         0.852653        0.041018        3.271714       0.428855\n",
            "35         0.831815        0.036927        3.266255       0.428812\n",
            "36         0.810852        0.031442        3.258712       0.431901\n",
            "37         0.784156        0.024416        3.259457       0.437192\n",
            "38         0.765718        0.026063        3.258147       0.440075\n",
            "39         0.738290        0.025509        3.259074       0.441318\n",
            "40         0.713401        0.012099        3.260375       0.444337\n",
            "41         0.700882        0.016973        3.255900       0.442181\n",
            "42         0.686943        0.017920        3.261377       0.441252\n",
            "43         0.664417        0.025525        3.257312       0.438848\n",
            "44         0.641893        0.022671        3.257818       0.441389\n",
            "45         0.625918        0.013790        3.260048       0.439502\n",
            "46         0.607496        0.013050        3.259309       0.439397\n",
            "47         0.597147        0.007861        3.259014       0.437781\n",
            "48         0.588216        0.008274        3.257008       0.440021\n",
            "49         0.570306        0.008310        3.256143       0.439984\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}